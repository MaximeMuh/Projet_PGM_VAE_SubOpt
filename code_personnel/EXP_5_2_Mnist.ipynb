{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Expérience 5.2 (papier) — Comparer les gaps pour un VAE amorti (MNIST)\n",
        "\n",
        "Ici, on reproduit l’expérience 5.2 du papier : on entraîne **un VAE complet** (encodeur + décodeur) avec une famille amortie donnée $q_\\phi(z|x)$, puis on mesure les trois gaps sur un petit subset fixe.\n",
        "\n",
        "On compare deux modèles amortis :\n",
        "- **FFG (Gaussian)** : $q_\\phi(z|x)$ est une gaussienne factorisée,\n",
        "- **Flow** : $q_\\phi(z|x)$ est un posterior plus flexible (avec transformations inversibles).\n",
        "- Pour fashion mnist, on étend au **Contextual Flow**.\n",
        "\n",
        "La procédure est :\n",
        "1) On fixe un subset de données (ici 10 points) pour comparer vite et de façon stable.  \n",
        "2) Pour chaque modèle amorti (FFG puis Flow), on entraîne le VAE sur tout le train set.  \n",
        "3) Sur le subset, on estime $\\log \\hat p(x)$ (IWAE et aussi AIS ; on garde le max, comme dans le papier).  \n",
        "4) On calcule $\\mathcal{L}[q]$ : l’ELBO **amorti** avec l’encodeur appris.  \n",
        "5) On calcule $\\mathcal{L}[q^*]$ en faisant une **optimisation locale** de $q$ pour chaque point (on le fait dans deux familles possibles : Gaussien et Flow).  \n",
        "\n",
        "Ensuite, on déduit les gaps :\n",
        "- approximation gap : $\\log \\hat p(x) - \\mathcal{L}[q^*]$  \n",
        "- amortization gap : $\\mathcal{L}[q^*] - \\mathcal{L}[q]$  \n",
        "- inference gap : $\\log \\hat p(x) - \\mathcal{L}[q]$\n",
        "\n",
        "Ce qu’on veut observer : même si le modèle Flow est plus expressif, est-ce que le gain vient surtout de la **réduction de l’approximation gap**, ou est-ce qu’on réduit aussi l’**amortization gap** (donc l’encodeur généralise mieux) ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, numpy as np, csv, time\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from tqdm import tqdm\n",
        "\n",
        "base_dir = Path.cwd().parent\n",
        "sys.path.insert(0, str(base_dir / 'models'))\n",
        "sys.path.insert(0, str(base_dir / 'models' / 'utils'))\n",
        "\n",
        "\n",
        "from vae_2 import VAE\n",
        "from inference_net import standard\n",
        "from distributions import Gaussian, Flow\n",
        "from optimize_local_q import optimize_local_q_dist\n",
        "from ais3 import test_ais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset and Device "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On charge le dataset MNIST binarisé (train / valid / test) depuis le fichier .npz\n",
        "data = np.load('binarized_mnist.npz')\n",
        "\n",
        "# On récupère les trois splits directement depuis les clés du fichier\n",
        "train_x, valid_x, test_x = data['train_data'], data['valid_data'], data['test_data']\n",
        "\n",
        "# On fixe les dimensions du problème :\n",
        "# x_size = 28*28 = 784 pixels, z_size = 50 dimensions latentes (comme dans le papier)\n",
        "x_size, z_size = 784, 50\n",
        "\n",
        "print(f\" Train {train_x.shape}, Valid {valid_x.shape}, Test {test_x.shape}\")\n",
        "\n",
        "device = torch.device(\n",
        "    'cuda' if torch.cuda.is_available()\n",
        "    else 'mps' if torch.backends.mps.is_available()\n",
        "    else 'cpu'\n",
        ")\n",
        "\n",
        "print(f\"Device actif : {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Définitions des fonctions utiles \n",
        "- de train \n",
        "- d'évaluations des gaps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_logp_IWAE(model, X, K=10000, batch_size=100):\n",
        "    # On estime log p(x) avec IWAE\n",
        "    # torch.no_grad() car on ne garde pas le graphe de gradient, c’est juste de l’évaluation.\n",
        "    vals = []\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        torch.from_numpy(X).float(), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    for xb in loader:\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        # forward2 renvoie typiquement l'IWAE bound pour k=K\n",
        "        v, _, _ = model.forward2(xb, k=K)\n",
        "        vals.append(v.item())\n",
        "\n",
        "    # On renvoie la moyenne sur tous les batchs\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "\n",
        "def estimate_logp_AIS(model, X, K=100, T=500, batch_size=100):\n",
        "    # On estime log p(x) via AIS \n",
        "    # Ici K = nb de particules AIS, T = nb d'intermédiaires\n",
        "    vals = []\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        xb = X[i:i+batch_size]\n",
        "        try:\n",
        "            # test_ais renvoie un estimateur de log p(x) \n",
        "            est = test_ais(model, xb, xb.shape[0], 0, K, T)\n",
        "\n",
        "            vals.append(float(est.item() if torch.is_tensor(est) else est))\n",
        "        except Exception as e:\n",
        "            print(\"AIS batch fail:\", e)\n",
        "\n",
        "    return float(np.mean(vals)) if vals else np.nan\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def amortized_elbo(model, X, batch_size=100):\n",
        "    # On calcule L[q_phi] : l'ELBO amorti du modèle (encoder appris).\n",
        "    vals = []\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        torch.from_numpy(X).float(), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    for xb in loader:\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        # forward renvoie l'ELBO\n",
        "        v, _, _ = model.forward(xb, k=1, warmup=1.0)\n",
        "        vals.append(v.item())\n",
        "\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "\n",
        "\n",
        "def locally_optimized_elbo(model, X, q_star_class, n_points=10):\n",
        "    # On calcule L[q*] : ELBO avec une distribution locale q optimisée par point.\n",
        "    # Idée : on garde p_theta fixé (le decoder) et on optimise q sur chaque x.\n",
        "    hyper = model.hyper_params\n",
        "    vals = []\n",
        "\n",
        "    # On ne le fait que sur quelques points car c'est coûteux.\n",
        "    for i in tqdm(range(min(n_points, len(X))), desc=f\"q* = {q_star_class.__name__}\"):\n",
        "        # On prend un seul point x (shape [1, x_size])\n",
        "        x = torch.from_numpy(X[i]).float().view(1, -1).to(device)\n",
        "        logpost = lambda z: model.logposterior_func2(x=x, z=z)\n",
        "\n",
        "        # On instancie une q locale de la famille demandée (FFG / Flow / etc.)\n",
        "        q_local = q_star_class(hyper).to(device)\n",
        "\n",
        "        # On \"warm-start\" q_local depuis la q amortie du modèle\n",
        "        try:\n",
        "            q_local.load_state_dict(model.hyper_params[\"q\"].state_dict())\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Optimisation locale : on maximise l'ELBO pour ce x en mettant à jour q_local.\n",
        "        Lqs, _ = optimize_local_q_dist(logpost, hyper, x, q_local)\n",
        "\n",
        "        # On stocke la valeur finale (ELBO optimisé pour ce point).\n",
        "        vals.append(float(Lqs.item()))\n",
        "\n",
        "    # On renvoie la moyenne des L[q*] sur les points testés.\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "\n",
        "def build_vae(q_class):\n",
        "    # On construit un VAE avec une architecture fixe (encoder/decoder MLP).\n",
        "    # Ici on utilise une sortie encoder de taille 2*z_size (mu + logvar).\n",
        "    enc_arch = [[x_size,200],[200,200],[200,2*z_size]]\n",
        "    dec_arch = [[z_size,200],[200,200],[200,x_size]]\n",
        "\n",
        "    hyper = {\n",
        "        \"x_size\": x_size, \"z_size\": z_size,\n",
        "        \"act_func\": F.elu,  # activation (ELU)\n",
        "        \"encoder_arch\": enc_arch,\n",
        "        \"decoder_arch\": dec_arch,\n",
        "        \"q_dist\": standard,  \n",
        "        \"cuda\": int(device.type == \"mps\"),  \n",
        "        \"hnf\": 0\n",
        "    }\n",
        "\n",
        "    # On crée la distribution variationnelle q (Gaussian, Flow, etc.)\n",
        "    q = q_class(hyper)\n",
        "    hyper[\"q\"] = q\n",
        "\n",
        "    m = VAE(hyper).to(device)\n",
        "    m.hyper_params = hyper\n",
        "    return m\n",
        "\n",
        "\n",
        "def train_model(model, X, epochs=300, batch_size=100, lr=1e-3):\n",
        "    X_t = torch.from_numpy(X).float()\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(X_t, torch.zeros(len(X))),\n",
        "        batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    # On optimise à la fois l'encoder (q_dist) et le decoder (generator).\n",
        "    opt = optim.Adam(\n",
        "        list(model.q_dist.parameters()) + list(model.generator.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "\n",
        "    # Warm-up : on fait monter progressivement le poids du KL au début du training.\n",
        "    warm_T = 50.0\n",
        "    global_step = 0\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            global_step += 1\n",
        "\n",
        "            # warm passe de ~0 à 1 sur les 50 premières itérations \n",
        "            warm = min(global_step / warm_T, 1.0)\n",
        "\n",
        "            # On calcule l'ELBO\n",
        "            elbo, _, _ = model.forward(xb, k=1, warmup=warm)\n",
        "            loss = -elbo  # on minimise -ELBO\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        if ep % 50 == 0:\n",
        "            print(f\"[{ep}/{epochs}] ELBO={elbo.item():.3f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Définitions de la fonction de run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_exp_5_2(train_x, test_x):\n",
        "    out_dir = Path(\"exp52_results_VF\")\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    csv_path = out_dir / \"exp52_results_VF.csv\"\n",
        "\n",
        "    if not csv_path.exists():\n",
        "        with open(csv_path, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                \"model_family\", \"q_star\", \"logp\",\n",
        "                \"Lq\", \"Lq_star\",\n",
        "                \"approx_gap\", \"amort_gap\", \"infer_gap\"\n",
        "            ])\n",
        "\n",
        "    # On définit les deux modèles amortis qu’on veut comparer : Gaussian (FFG) vs Flow.\n",
        "    models = [(\"FFG\", Gaussian), (\"Flow\", Flow)]\n",
        "\n",
        "    # On fixe un subset aléatoire pour comparer les modèles équitablement.\n",
        "    rng = np.random.default_rng(0)\n",
        "    subset = train_x[rng.choice(len(train_x), size=10, replace=False)]\n",
        "\n",
        "    # On boucle sur les deux familles : on entraîne un modèle amorti, puis on évalue les gaps.\n",
        "    for name, q_class in models:\n",
        "\n",
        "        # On construit le VAE avec la famille de posterior amorti choisie (FFG ou Flow).\n",
        "        model = build_vae(q_class)\n",
        "\n",
        "        # On entraîne le modèle sur tout le train set \n",
        "        model = train_model(model, train_x, epochs=300)\n",
        "\n",
        "        # On estime log p(x) sur le subset avec deux estimateurs (IWAE et AIS).\n",
        "        logp_iwae = estimate_logp_IWAE(model, subset, K=10000)\n",
        "        logp_ais  = estimate_logp_AIS(model, subset, K=100, T=500)\n",
        "\n",
        "        # On choisit la meilleure estimation comme dans le papier\n",
        "        logp = max(logp_iwae, logp_ais if not np.isnan(logp_ais) else -1e9)\n",
        "\n",
        "        # On calcule l’ELBO amorti L[q_phi] sur le subset.\n",
        "        Lq = amortized_elbo(model, subset)\n",
        "\n",
        "        # On calcule l’ELBO localement optimisé avec q* dans la famille Gaussian.\n",
        "        Lqs_FFG = locally_optimized_elbo(model, subset, Gaussian)\n",
        "\n",
        "        # On calcule l’ELBO localement optimisé avec q* dans la famille Flow.\n",
        "        Lqs_Flow = locally_optimized_elbo(model, subset, Flow)\n",
        "\n",
        "        # On regroupe le calcul des trois gaps (approximation, amortization, total).\n",
        "        def gaps(logp, Lq, Lqs):\n",
        "            return logp - Lqs, Lqs - Lq, logp - Lq\n",
        "\n",
        "        g_FFG  = gaps(logp, Lq, Lqs_FFG)\n",
        "        g_Flow = gaps(logp, Lq, Lqs_Flow)\n",
        "\n",
        "        print(\"\\n=== Résultats ===\")\n",
        "        print(f\"log p̂(x)     = {logp:.3f}\")\n",
        "        print(f\"L[q]         = {Lq:.3f}\")\n",
        "        print(f\"L[q*_FFG]    = {Lqs_FFG:.3f}\")\n",
        "        print(f\"L[q*_Flow]   = {Lqs_Flow:.3f}\")\n",
        "        print(f\"Gaps FFG     = {g_FFG}\")\n",
        "        print(f\"Gaps Flow    = {g_Flow}\")\n",
        "\n",
        "        with open(csv_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([name, \"FFG\",  logp, Lq, Lqs_FFG,  *g_FFG])\n",
        "            writer.writerow([name, \"Flow\", logp, Lq, Lqs_Flow, *g_Flow])\n",
        "\n",
        "run_exp_5_2(train_x, test_x)\n",
        "\n",
        "print(\" Résultats sauvés dans exp52_results/exp52_results.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
