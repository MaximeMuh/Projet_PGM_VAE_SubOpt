{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Expérience 5.3 (papier) — *Decoder frozen* + comparaison des familles de postérieurs\n",
        "\n",
        "Ici, on reproduit l’expérience 5.3 du papier : l’idée est de **geler le décodeur** pour que le vrai posterior $p_\\theta(z|x)$ reste le même, et qu’on compare uniquement l’effet de la famille variationnelle $q(z|x)$.\n",
        "\n",
        "Concrètement, on fait :\n",
        "1) On entraîne un VAE de base (ici avec $q$ Gaussien), puis on **sauvegarde le décodeur**.  \n",
        "2) On recharge ce même décodeur (figé) et on entraîne seulement des **encodeurs “small”** avec différentes familles :  \n",
        "- Gaussien (FFG),  \n",
        "- Flow,  \n",
        "- ContextualFlow.  \n",
        "\n",
        "Ensuite, sur un **subset fixe** (par ex. 100 points de test), on calcule :\n",
        "- $\\log \\hat p(x)$ (IWAE, et parfois AIS aussi ; on garde le max),\n",
        "- $\\mathcal{L}[q]$ (ELBO amorti : encodeur),\n",
        "- $\\mathcal{L}[q^*]$ (ELBO “local” : on optimise $q$ pour chaque point).\n",
        "\n",
        "Et on en déduit les gaps :\n",
        "- approximation gap : $\\log \\hat p(x) - \\mathcal{L}[q^*]$  \n",
        "- amortization gap : $\\mathcal{L}[q^*] - \\mathcal{L}[q]$  \n",
        "- inference gap : $\\log \\hat p(x) - \\mathcal{L}[q]$\n",
        "\n",
        "Ce qu’on regarde : est-ce qu’un posterior plus flexible (Flow / ContextualFlow) réduit **seulement** l’approximation gap, ou bien aussi l’amortization gap (donc l’encodeur généralise mieux), tout en gardant exactement le même décodeur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, numpy as np, csv\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "base_dir = Path.cwd().parent\n",
        "sys.path.insert(0, str(base_dir / 'models'))\n",
        "sys.path.insert(0, str(base_dir / 'models' / 'utils'))\n",
        "\n",
        "from vae_2 import VAE\n",
        "from inference_net import standard\n",
        "from distributions import Gaussian, Flow, ContextualFlow\n",
        "from optimize_local_q import optimize_local_q_dist\n",
        "from ais3 import test_ais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset & Device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = np.load('binarized_mnist.npz')\n",
        "train_x, valid_x, test_x = data['train_data'], data['valid_data'], data['test_data']\n",
        "print(f\"Train {train_x.shape}, Valid {valid_x.shape}, Test {test_x.shape}\")\n",
        "\n",
        "device = torch.device(\n",
        "    'cuda' if torch.cuda.is_available()\n",
        "    else 'mps' if torch.backends.mps.is_available()\n",
        "    else 'cpu'\n",
        ")\n",
        "print(f\"Device actif : {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Définitions des fonctions utiles \n",
        "- de train \n",
        "- d'évaluations des gaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_logp_IWAE(model, X, K=5000, batch_size=64):\n",
        "    # On estime log p(x) via IWAE(K) et on moyenne sur les batches.\n",
        "    vals = []\n",
        "    loader = torch.utils.data.DataLoader(torch.from_numpy(X).float(), batch_size=batch_size, shuffle=False)\n",
        "    for xb in loader:\n",
        "        xb = xb.to(device)\n",
        "        val, _, _ = model.forward2(xb, k=K)\n",
        "        vals.append(val.item())\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "def estimate_logp_AIS(model, X, K=50, n_intermediate=500, batch_size=200):\n",
        "    # On estime log p(x) via AIS, par batches\n",
        "    vals = []\n",
        "    model.eval()\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        xb = X[i:i+batch_size]\n",
        "        if not isinstance(xb, np.ndarray):\n",
        "            xb = xb.detach().cpu().numpy()\n",
        "        try:\n",
        "            est = test_ais(model=model, data_x=xb, batch_size=xb.shape[0],\n",
        "                           display=0, k=K, n_intermediate_dists=n_intermediate)\n",
        "            if torch.is_tensor(est):\n",
        "                est = est.item()\n",
        "            vals.append(float(est))\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ AIS batch {i//batch_size}: {e}\")\n",
        "    return float(np.mean(vals)) if vals else np.nan\n",
        "\n",
        "def estimate_logp(model, X, method=\"IWAE\", K_iwae=5000, K_ais=50, n_intermediate=500):\n",
        "    # Petit wrapper pratique : on choisit IWAE, AIS, ou les deux.\n",
        "    if method == \"IWAE\":\n",
        "        return estimate_logp_IWAE(model, X, K_iwae)\n",
        "    elif method == \"AIS\":\n",
        "        return estimate_logp_AIS(model, X, K_ais, n_intermediate)\n",
        "    elif method == \"BOTH\":\n",
        "        return {\n",
        "            \"IWAE\": estimate_logp_IWAE(model, X, K_iwae),\n",
        "            \"AIS\": estimate_logp_AIS(model, X, K_ais, n_intermediate)\n",
        "        }\n",
        "\n",
        "@torch.no_grad()\n",
        "def amortized_elbo(model, X, k=1, batch_size=64):\n",
        "    # On calcule L[q_phi] (ELBO amorti) : passage forward standard avec warmup=1.\n",
        "    vals = []\n",
        "    loader = torch.utils.data.DataLoader(torch.from_numpy(X).float(), batch_size=batch_size, shuffle=False)\n",
        "    for xb in loader:\n",
        "        xb = xb.to(device)\n",
        "        elbo, _, _ = model.forward(xb, k=k, warmup=1.0)\n",
        "        vals.append(elbo.item())\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "def locally_optimized_elbo(model, X, n_points=10):\n",
        "    # On calcule L[q*] : pour quelques points, on ré-optimise localement une q du même type que celle du modèle.\n",
        "    vals = []\n",
        "    q_class = type(model.hyper_params['q'])\n",
        "    for i in range(min(n_points, len(X))):\n",
        "        x = torch.from_numpy(X[i]).float().view(1, -1).to(device)\n",
        "        logpost = lambda z: model.logposterior_func2(x=x, z=z)\n",
        "        q_local = q_class(model.hyper_params).to(device)\n",
        "        try:\n",
        "            q_local.load_state_dict(model.hyper_params['q'].state_dict(), strict=False)\n",
        "        except:\n",
        "            pass\n",
        "        vae_star, _ = optimize_local_q_dist(logpost, model.hyper_params, x, q_local)\n",
        "        vals.append(float(vae_star.item()))\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "def train_base_vae_ffg(train_x, max_epochs=500, batch_size=64, z_size=50):\n",
        "    # On entraîne un VAE \"base\" avec qFFG, puis on sauvegarde decoder + encoder (utile pour geler le decoder ensuite).\n",
        "    x_size = train_x.shape[1]\n",
        "    hyper = {\n",
        "        'x_size': x_size, 'z_size': z_size, 'act_func': F.elu,\n",
        "        'encoder_arch': [[x_size,200],[200,200],[200,2*z_size]],\n",
        "        'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n",
        "        'q_dist': standard, 'cuda': int(device.type=='cuda'), 'hnf': 0,\n",
        "        'context_size': 0\n",
        "    }\n",
        "    q = Gaussian(hyper)\n",
        "    hyper['q'] = q\n",
        "    model = VAE(hyper).to(device)\n",
        "    X = torch.from_numpy(train_x).float()\n",
        "    y = torch.zeros(len(X))\n",
        "    loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X, y), batch_size=batch_size, shuffle=True)\n",
        "    opt = optim.Adam(list(model.q_dist.parameters()) + list(model.generator.parameters()), lr=1e-3)\n",
        "\n",
        "    warmup_T = 100.0\n",
        "    total_epochs = 0\n",
        "    for ep in range(1, max_epochs + 1):\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            opt.zero_grad()\n",
        "            warm = min(total_epochs / warmup_T, 1.0)\n",
        "            elbo, _, _ = model.forward(xb, k=1, warmup=warm)\n",
        "            (-elbo).backward()\n",
        "            opt.step()\n",
        "        total_epochs += 1\n",
        "        if ep % 10 == 0:\n",
        "            print(f\"[BASE] epoch {ep}/{max_epochs} ELBO={elbo.item():.3f}\")\n",
        "\n",
        "    torch.save(model.generator.state_dict(), \"decoder_base_qFFG.pt\")\n",
        "    torch.save(model.q_dist.state_dict(), \"encoder_base_qFFG.pt\")\n",
        "    print(\" Base model saved\")\n",
        "    return \"decoder_base_qFFG.pt\", \"encoder_base_qFFG.pt\", hyper\n",
        "\n",
        "def train_encoder_only(train_x, frozen_decoder_path, q_family, q_name, base_hyper,\n",
        "                       max_epochs=500, batch_size=64):\n",
        "    # Ici on refait uniquement l’encodeur (q_family), en gelant le decoder du modèle base.\n",
        "    x_size, z_size = base_hyper['x_size'], base_hyper['z_size']\n",
        "    hyper = dict(base_hyper)\n",
        "    \n",
        "    # On adapte l’architecture encoder selon la famille (Gaussian / Flow / ContextualFlow).\n",
        "    if q_family is ContextualFlow:\n",
        "        context_size = 128\n",
        "        hyper['context_size'] = context_size\n",
        "        output_size = 2 * z_size + context_size\n",
        "        enc_arch = [[x_size, 100], [100, output_size]]\n",
        "    elif q_family is Gaussian:\n",
        "        enc_arch = [[x_size, z_size], [z_size, 2*z_size]]\n",
        "        hyper['context_size'] = 0\n",
        "    else: \n",
        "        enc_arch = [[x_size, 100], [100, 2*z_size]]\n",
        "        hyper['context_size'] = 0\n",
        "        \n",
        "    hyper['encoder_arch'] = enc_arch\n",
        "    q = q_family(hyper)\n",
        "    hyper['q'] = q\n",
        "\n",
        "    model = VAE(hyper).to(device)\n",
        "\n",
        "    # On charge le decoder entraîné, puis on le fige pour isoler l’effet de q(z|x).\n",
        "    model.generator.load_state_dict(torch.load(frozen_decoder_path, map_location=device))\n",
        "    for p in model.generator.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # On optimise seulement les paramètres de q_dist (l’encodeur).\n",
        "    opt = optim.Adam(model.q_dist.parameters(), lr=1e-3)\n",
        "\n",
        "    X = torch.from_numpy(train_x).float()\n",
        "    y = torch.zeros(len(X))\n",
        "    loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X, y), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for ep in range(1, max_epochs + 1):\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            opt.zero_grad()\n",
        "            elbo, _, _ = model.forward(xb, k=1, warmup=1.0)\n",
        "            (-elbo).backward()\n",
        "            opt.step()\n",
        "        if ep % 10 == 0:\n",
        "            print(f\"[{q_name}] epoch {ep}/{max_epochs} ELBO={elbo.item():.3f}\")\n",
        "\n",
        "    # On sauvegarde l’encodeur ré-entraîné.\n",
        "    enc_path = f\"encoder_{q_name}.pt\"\n",
        "    torch.save(model.q_dist.state_dict(), enc_path)\n",
        "    print(f\" Saved {enc_path}\")\n",
        "    return enc_path, hyper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fonction de run de l'expérience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_exp_5_3(train_x, eval_x,\n",
        "                K_logp=10000, n_local=25,\n",
        "                use_pretrained=False, pretrained_paths=None,\n",
        "                use_ais=False, K_ais=50, T_ais=200):\n",
        "\n",
        "    # On fixe un subset d’évaluation pour avoir une comparaison stable et rapide.\n",
        "    rng = np.random.default_rng(0)\n",
        "    idx_subset = rng.choice(len(eval_x), size=min(100, len(eval_x)), replace=False)\n",
        "    subset = eval_x[idx_subset]\n",
        "\n",
        "    # On prépare un dossier + un CSV pour logger les résultats.\n",
        "    out_dir = Path(\"exp53_logs\")\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "    csv_path = out_dir / \"exp53_results.csv\"\n",
        "    if not csv_path.exists():\n",
        "        with open(csv_path, \"w\", newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"family\", \"logp\", \"Lq\", \"Lq*\", \"approx_gap\", \"amort_gap\", \"infer_gap\"])\n",
        "\n",
        "    # On a deux modes :\n",
        "    # (1) use_pretrained=True : on charge un decoder déjà entraîné + des encodeurs \"small\" déjà entraînés\n",
        "    # (2) use_pretrained=False : on entraîne tout (base VAE + encodeurs small)\n",
        "    if use_pretrained:\n",
        "        print(\" Mode Pretrained activé : Chargement des poids existants...\")\n",
        "\n",
        "        # On récupère les chemins de poids (décodeur + encodeurs).\n",
        "        dec_path = pretrained_paths['decoder']\n",
        "        enc_ffg_small = pretrained_paths['enc_ffg']\n",
        "        enc_flow_small = pretrained_paths['enc_flow']\n",
        "        enc_cflow_small = pretrained_paths.get('enc_cflow', None)\n",
        "        \n",
        "        # On reconstruit les hyperparamètres (architectures) qui doivent matcher les poids sauvegardés.\n",
        "        # Ici on fait une \"base_hyper\" commune (le décodeur est le même pour les trois).\n",
        "        base_hyper = {\n",
        "            'x_size': eval_x.shape[1], 'z_size': 50, 'act_func': F.elu,\n",
        "            'decoder_arch': [[50,200],[200,200],[200,eval_x.shape[1]]],\n",
        "            'q_dist': standard, 'cuda': int(device.type=='cuda'), 'hnf': 0, 'context_size': 0\n",
        "        }\n",
        "        \n",
        "        # On crée ensuite un hyper spécifique à chaque famille pour que l’encodeur ait la bonne forme.\n",
        "        hyper_ffg = dict(base_hyper); hyper_ffg['encoder_arch'] = [[base_hyper['x_size'], 50], [50, 100]]\n",
        "        hyper_flow = dict(base_hyper); hyper_flow['encoder_arch'] = [[base_hyper['x_size'], 100], [100, 100]]\n",
        "        hyper_cflow = dict(base_hyper); hyper_cflow['encoder_arch'] = [[base_hyper['x_size'], 100], [100, 100 + 128]]; hyper_cflow['context_size'] = 128\n",
        "        \n",
        "    else:\n",
        "        # On entraîne un VAE base (qFFG) et on sauvegarde le décodeur.\n",
        "        # Puis on gèle ce décodeur et on entraîne des encodeurs \"small\" de familles différentes.\n",
        "        dec_path, _, base_hyper = train_base_vae_ffg(train_x, max_epochs=100)\n",
        "        \n",
        "        enc_ffg_small,  hyper_ffg   = train_encoder_only(train_x, dec_path, Gaussian, 'qFFG_small', base_hyper, max_epochs=80)\n",
        "        enc_flow_small, hyper_flow  = train_encoder_only(train_x, dec_path, Flow,   'qFlow_small', base_hyper, max_epochs=80)\n",
        "        enc_cflow_small, hyper_cflow = train_encoder_only(train_x, dec_path, ContextualFlow, 'qCFlow_small', base_hyper, max_epochs=80)\n",
        "\n",
        "    def build_model(enc_path, hyper):\n",
        "        # On reconstruit un VAE \n",
        "        # puis on charge le decoder gelé et les poids de l’encodeur correspondant.\n",
        "        hyper = dict(hyper)\n",
        "        \n",
        "        # On déduit la famille q(z|x) à partir du nom du checkpoint \n",
        "        if \"CFlow\" in enc_path:\n",
        "            q_class = ContextualFlow\n",
        "        elif \"FFG\" in enc_path:\n",
        "            q_class = Gaussian\n",
        "        else:\n",
        "            q_class = Flow\n",
        "            \n",
        "        q = q_class(hyper)\n",
        "        hyper['q'] = q\n",
        "        m = VAE(hyper).to(device)\n",
        "\n",
        "        # On charge le même décodeur pour toutes les familles (expérience \"decoder frozen\").\n",
        "        m.generator.load_state_dict(torch.load(dec_path, map_location=device))\n",
        "        \n",
        "        # On charge les poids de l’encodeur (qui doivent matcher l’arch de hyper['encoder_arch']).\n",
        "        m.q_dist.load_state_dict(torch.load(enc_path, map_location=device))\n",
        "        m.hyper_params = hyper\n",
        "        return m\n",
        "\n",
        "    # On construit les trois modèles \"small\" (même decoder, encodeurs différents).\n",
        "    model_ffg  = build_model(enc_ffg_small, hyper_ffg)\n",
        "    model_flow = build_model(enc_flow_small, hyper_flow)\n",
        "    model_cflow = build_model(enc_cflow_small, hyper_cflow)\n",
        "\n",
        "    # Estimation de log p(x) : soit IWAE seul, soit IWAE + AIS puis on garde le max\n",
        "    if use_ais:\n",
        "        print(\" Estimation log p(x) avec AIS + IWAE ...\")\n",
        "        logp_ffg_all  = estimate_logp(model_ffg,  subset, method=\"BOTH\", K_iwae=K_logp, K_ais=K_ais, n_intermediate=T_ais)\n",
        "        logp_flow_all = estimate_logp(model_flow, subset, method=\"BOTH\", K_iwae=K_logp, K_ais=K_ais, n_intermediate=T_ais)\n",
        "        logp_cflow_all = estimate_logp(model_cflow, subset, method=\"BOTH\", K_iwae=K_logp, K_ais=K_ais, n_intermediate=T_ais)\n",
        "        \n",
        "        logp_ffg  = max(v for v in logp_ffg_all.values()  if v is not None)\n",
        "        logp_flow = max(v for v in logp_flow_all.values() if v is not None)\n",
        "        logp_cflow = max(v for v in logp_cflow_all.values() if v is not None)\n",
        "    else:\n",
        "        logp_ffg  = estimate_logp(model_ffg,  subset, method=\"IWAE\", K_iwae=K_logp)\n",
        "        logp_flow = estimate_logp(model_flow, subset, method=\"IWAE\", K_iwae=K_logp)\n",
        "        logp_cflow = estimate_logp(model_cflow, subset, method=\"IWAE\", K_iwae=K_logp)\n",
        "\n",
        "    # On calcule L[q] (amorti) et L[q*] (optimisation locale) pour chaque famille.\n",
        "    Lq_ffg, Lq_flow, Lq_cflow = amortized_elbo(model_ffg, subset), amortized_elbo(model_flow, subset), amortized_elbo(model_cflow, subset)\n",
        "    Lqstar_ffg, Lqstar_flow, Lqstar_cflow = locally_optimized_elbo(model_ffg, subset, n_points=n_local), locally_optimized_elbo(model_flow, subset, n_points=n_local), locally_optimized_elbo(model_cflow, subset, n_points=n_local)\n",
        "\n",
        "    # On en déduit les trois gaps : approximation, amortization, total inference.\n",
        "    def compute_gaps(logp, Lq, Lqs):\n",
        "        return logp - Lqs, Lqs - Lq, logp - Lq\n",
        "\n",
        "    gaps_ffg = compute_gaps(logp_ffg, Lq_ffg, Lqstar_ffg)\n",
        "    gaps_flow = compute_gaps(logp_flow, Lq_flow, Lqstar_flow)\n",
        "    gaps_cflow = compute_gaps(logp_cflow, Lq_cflow, Lqstar_cflow)\n",
        "\n",
        "    # On affiche et on log dans le CSV.\n",
        "    for fam, logp, Lq, Lqs, g in [\n",
        "        (\"qFFG_small\", logp_ffg, Lq_ffg, Lqstar_ffg, gaps_ffg),\n",
        "        (\"qFlow_small\", logp_flow, Lq_flow, Lqstar_flow, gaps_flow),\n",
        "        (\"qCFlow_small\", logp_cflow, Lq_cflow, Lqstar_cflow, gaps_cflow)\n",
        "    ]:\n",
        "        A, M, I = g\n",
        "        print(f\"\\n{fam}: logp={logp:.2f}, Lq*={Lqs:.2f}, Lq={Lq:.2f}, A={A:.2f}, M={M:.2f}, I={I:.2f}\")\n",
        "        with open(csv_path, \"a\", newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([fam, round(logp, 2), round(Lq, 2), round(Lqs, 2), round(A, 2), round(M, 2), round(I,2)])\n",
        "\n",
        "    print(f\"\\n Résultats sauvegardés dans {csv_path}\")\n",
        "\n",
        "    return {\"ffg\": (logp_ffg, Lq_ffg, Lqstar_ffg, gaps_ffg),\n",
        "            \"flow\": (logp_flow, Lq_flow, Lqstar_flow, gaps_flow),\n",
        "            \"cflow\": (logp_cflow, Lq_cflow, Lqstar_cflow, gaps_cflow)}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ici on définit les chemins vers les checkpoints qu’on veut charger en mode pretrained.\n",
        "    my_pretrained_paths = {\n",
        "        'decoder': 'decoder_base_qFFG.pt',\n",
        "        'enc_ffg': 'encoder_qFFG_small.pt',      \n",
        "        'enc_flow': 'encoder_qFlow_small.pt',\n",
        "        'enc_cflow': 'encoder_qCFlow_small.pt'\n",
        "    }\n",
        "\n",
        "    # On lance l’expérience 5.3 : decoder figé + encodeurs small, et on compare les gaps.\n",
        "    results = run_exp_5_3(\n",
        "        train_x=train_x,\n",
        "        eval_x=test_x,\n",
        "        K_logp=5000,\n",
        "        n_local=15,\n",
        "        use_pretrained=True,                 \n",
        "        pretrained_paths=my_pretrained_paths, \n",
        "        use_ais=True, K_ais=20, T_ais=100\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
