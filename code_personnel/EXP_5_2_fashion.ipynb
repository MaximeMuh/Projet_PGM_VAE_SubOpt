{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Expérience 5.2 (papier) — Comparer les gaps pour un VAE amorti + Extension Contextual Flow (Fashion-MNIST)\n",
        "\n",
        "Ici, on reproduit l’expérience 5.2 du papier : on entraîne **un VAE complet** (encodeur + décodeur) avec une famille amortie donnée $q_\\phi(z|x)$, puis on mesure les trois gaps sur un petit subset fixe.\n",
        "\n",
        "On compare deux modèles amortis :\n",
        "- **FFG (Gaussian)** : $q_\\phi(z|x)$ est une gaussienne factorisée,\n",
        "- **Flow** : $q_\\phi(z|x)$ est un posterior plus flexible (avec transformations inversibles).\n",
        "- **Contextual Flow** : même idée, mais les paramètres du flow dépendent explicitement de $x$ (via l’encodeur).  \n",
        "  On écrit :\n",
        "  $$\n",
        "  z = f_{\\lambda(x)}(z_0), \\qquad z_0 \\sim \\mathcal N(0,I),\n",
        "  $$\n",
        "  avec $\\lambda(x)$ produit par le réseau d’inférence.  \n",
        "  Donc :\n",
        "  $$\n",
        "  \\log q_\\phi(z|x)=\\log q(z_0)-\\log\\left|\\det\\frac{\\partial f_{\\lambda(x)}}{\\partial z_0}\\right|.\n",
        "  $$\n",
        "  Donc la forme du posterior peut changer plus finement selon chaque entrée $x$.\n",
        "\n",
        "\n",
        "La procédure est :\n",
        "1) On fixe un subset de données pour comparer vite et de façon stable.  \n",
        "2) Pour chaque modèle amorti (FFG puis Flow), on entraîne le VAE sur tout le train set.  \n",
        "3) Sur le subset, on estime $\\log \\hat p(x)$ (IWAE et aussi AIS ; on garde le max, comme dans le papier).  \n",
        "4) On calcule $\\mathcal{L}[q]$ : l’ELBO **amorti** avec l’encodeur appris.  \n",
        "5) On calcule $\\mathcal{L}[q^*]$ en faisant une **optimisation locale** de $q$ pour chaque point (on le fait dans deux familles possibles : Gaussien et Flow).  \n",
        "\n",
        "Ensuite, on déduit les gaps :\n",
        "- approximation gap : $\\log \\hat p(x) - \\mathcal{L}[q^*]$  \n",
        "- amortization gap : $\\mathcal{L}[q^*] - \\mathcal{L}[q]$  \n",
        "- inference gap : $\\log \\hat p(x) - \\mathcal{L}[q]$\n",
        "\n",
        "Ce qu’on veut observer : même si le modèle Flow est plus expressif, est-ce que le gain vient surtout de la **réduction de l’approximation gap**, ou est-ce qu’on réduit aussi l’**amortization gap** (donc l’encodeur généralise mieux) ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, numpy as np, csv, time\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from tqdm import tqdm\n",
        "\n",
        "base_dir = Path.cwd().parent\n",
        "sys.path.insert(0, str(base_dir / 'models'))\n",
        "sys.path.insert(0, str(base_dir / 'models' / 'utils'))\n",
        "\n",
        "from vae_2 import VAE\n",
        "from inference_net import standard\n",
        "from distributions import Gaussian, Flow, ContextualFlow\n",
        "from optimize_local_q import optimize_local_q_dist\n",
        "from ais3 import test_ais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset and Device "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\n",
        "    'cuda' if torch.cuda.is_available()\n",
        "    else 'mps' if torch.backends.mps.is_available()\n",
        "    else 'cpu'\n",
        ")\n",
        "print(f'Device: {device}')\n",
        "\n",
        "\n",
        "# On charge le dataset Fashion-MNIST depuis un fichier .npz\n",
        "npz_path = 'fashion_mnist.npz'  \n",
        "data = np.load(npz_path)\n",
        "\n",
        "print(\"Clés:\", data.files)\n",
        "\n",
        "# On récupère les données d'entraînement\n",
        "train_data = data['X_train']\n",
        "\n",
        "# On récupère les données de test\n",
        "test_data  = data['X_test']\n",
        "\n",
        "print(f\"Train: {train_data.shape}, Test: {test_data.shape}\")\n",
        "\n",
        "\n",
        "# Ici, on prend tout le train et tout le test\n",
        "train_x = train_data           \n",
        "test_x  = test_data       \n",
        "\n",
        "# On fixe la dimension de l'entrée (x_size)\n",
        "# et la dimension latente (z_size)\n",
        "x_size, z_size = train_x.shape[1], 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Définitions des fonctions utiles \n",
        "- de train \n",
        "- d'évaluations des gaps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On désactive le calcul de gradient ici car on est en évaluation (plus rapide, moins de mémoire)\n",
        "@torch.no_grad()\n",
        "def estimate_logp_IWAE(model, X, K=10000, batch_size=100):\n",
        "    # On va stocker la valeur IWAE pour chaque batch puis moyenner\n",
        "    vals = []\n",
        "    \n",
        "    # On crée un DataLoader pour parcourir X par batches\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        torch.from_numpy(X).float(), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    \n",
        "    # On boucle sur les batches\n",
        "    for xb in loader:\n",
        "        xb = xb.to(device)\n",
        "        \n",
        "        # forward2 renvoie l'estimateur IWAE pour k=K samples\n",
        "        v, _, _ = model.forward2(xb, k=K)\n",
        "        vals.append(v.item())\n",
        "        \n",
        "    # On renvoie la moyenne sur tous les batches\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "\n",
        "def estimate_logp_AIS(model, X, K=100, T=500, batch_size=100):\n",
        "    # Ici on estime log p(x) avec AIS, batch par batch\n",
        "    vals = []\n",
        "    \n",
        "    for i in range(0, len(X), batch_size):\n",
        "        xb = X[i:i+batch_size]\n",
        "        try:\n",
        "            # test_ais renvoie une estimation de log p(x)\n",
        "            est = test_ais(model, xb, xb.shape[0], 0, K, T)\n",
        "            vals.append(float(est.item() if torch.is_tensor(est) else est))\n",
        "        except Exception as e:\n",
        "            print(\"AIS batch fail:\", e)\n",
        "    return float(np.mean(vals)) if vals else np.nan\n",
        "\n",
        "\n",
        "# On désactive le gradient car c'est aussi une évaluation\n",
        "@torch.no_grad()\n",
        "def amortized_elbo(model, X, batch_size=100):\n",
        "    # On calcule la ELBO amortie L[q_phi] sur X\n",
        "    vals = []\n",
        "    \n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        torch.from_numpy(X).float(), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    \n",
        "    for xb in loader:\n",
        "        xb = xb.to(device)\n",
        "        \n",
        "        # forward = ELBO standard avec k=1 sample, warmup=1.0 (KL pleinement actif)\n",
        "        v, _, _ = model.forward(xb, k=1, warmup=1.0)\n",
        "        vals.append(v.item())\n",
        "        \n",
        "    # On retourne la moyenne sur tous les batches\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "\n",
        "def locally_optimized_elbo(model, X, q_star_class, n_points=10):\n",
        "    # Ici on calcule L[q*] : on optimise localement une q (par point)\n",
        "    hyper = dict(model.hyper_params)\n",
        "    \n",
        "    # On adapte le contexte si on optimise une ContextualFlow\n",
        "    if q_star_class is ContextualFlow:\n",
        "        hyper['context_size'] = 128\n",
        "    else:\n",
        "        hyper['context_size'] = 0\n",
        "\n",
        "    # On stocke les ELBO optimisées par point\n",
        "    vals = []\n",
        "\n",
        "    # On ne fait pas tous les points (trop long), juste n_points\n",
        "    for i in tqdm(range(min(n_points, len(X))), desc=f\"q* = {q_star_class.__name__}\"):\n",
        "        x = torch.from_numpy(X[i]).float().view(1, -1).to(device)\n",
        "        \n",
        "        # On définit log p(x,z) - log q(z|x) via la fonction logposterior du modèle\n",
        "        logpost = lambda z: model.logposterior_func2(x=x, z=z)\n",
        "\n",
        "        # On instancie une distribution locale q(z|x) du bon type (Gaussian, Flow, ContextualFlow...)\n",
        "        q_local = q_star_class(hyper).to(device)\n",
        "\n",
        "        # On warm-start q_local avec les paramètres de la q amortie du modèle (si possible)\n",
        "        # strict=False car certaines clés peuvent manquer selon les familles\n",
        "        try:\n",
        "            q_local.load_state_dict(model.hyper_params[\"q\"].state_dict(), strict=False)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # On optimise q_local pour ce x (c'est ça qui approxime q*)\n",
        "        Lqs, _ = optimize_local_q_dist(logpost, hyper, x, q_local)\n",
        "        vals.append(float(Lqs.item()))\n",
        "\n",
        "    # On renvoie la moyenne des L[q*] sur les points évalués\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "\n",
        "def build_vae(q_class):\n",
        "    # On prépare la taille du \"contexte\" uniquement si on est en ContextualFlow\n",
        "    context_size = 128 if q_class is ContextualFlow else 0\n",
        "    \n",
        "    # L'encodeur doit prédire mean/logvar (2*z_size) + éventuellement le contexte\n",
        "    output_size = 2 * z_size + context_size\n",
        "    \n",
        "    # On choisit une architecture différente pour ContextualFlow \n",
        "    # car on a déjà plus de flexibilité via le flow conditionné\n",
        "    if q_class is ContextualFlow:\n",
        "        enc_arch = [[x_size, 100], [100, output_size]]\n",
        "    else:\n",
        "        enc_arch = [[x_size, 200], [200, 200], [200, output_size]]\n",
        "        \n",
        "    # Le décodeur reste identique pour comparer proprement\n",
        "    dec_arch = [[z_size, 200], [200, 200], [200, x_size]]\n",
        "\n",
        "    # Hyperparamètres du modèle\n",
        "    hyper = {\n",
        "        \"x_size\": x_size, \"z_size\": z_size,\n",
        "        \"act_func\": F.elu,  \n",
        "        \"encoder_arch\": enc_arch,\n",
        "        \"decoder_arch\": dec_arch,\n",
        "        \"q_dist\": standard,\n",
        "        \"cuda\": int(device.type == \"cuda\"),\n",
        "        \"hnf\": 0,\n",
        "        \"context_size\": context_size\n",
        "    }\n",
        "\n",
        "    # On instancie la distribution q \n",
        "    q = q_class(hyper)\n",
        "    hyper[\"q\"] = q\n",
        "    \n",
        "    # On construit le VAE complet et on garde hyper_params pour pouvoir refaire q_local plus tard\n",
        "    m = VAE(hyper).to(device)\n",
        "    m.hyper_params = hyper\n",
        "    return m\n",
        "\n",
        "\n",
        "def train_model(model, X, epochs=700, batch_size=100, lr=1e-3):\n",
        "    # On prépare le DataLoader pour l'entraînement\n",
        "    X_t = torch.from_numpy(X).float()\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(X_t, torch.zeros(len(X))),\n",
        "        batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    # On entraîne l'encodeur (q_dist) et le décodeur (generator) ensemble\n",
        "    opt = optim.Adam(\n",
        "        list(model.q_dist.parameters()) + list(model.generator.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "\n",
        "    # Warm-up KL : on augmente progressivement le poids du KL au début\n",
        "    warm_T = 100.0 \n",
        "    global_step = 0\n",
        "\n",
        "    # Boucle d'entraînement classique\n",
        "    for ep in range(1, epochs+1):\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            global_step += 1\n",
        "            \n",
        "            # warm augmente de 0 à 1 sur les 100 premières itérations environ\n",
        "            warm = min(global_step / warm_T, 1.0)\n",
        "\n",
        "            # forward retourne l'ELBO, on minimise -ELBO\n",
        "            elbo, _, _ = model.forward(xb, k=1, warmup=warm)\n",
        "            loss = -elbo\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        if ep % 50 == 0:\n",
        "            print(f\"[{ep}/{epochs}] ELBO={elbo.item():.3f}\")\n",
        "\n",
        "    # On renvoie le modèle entraîné\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Définitions de la fonction de run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_exp_5_2(train_x, test_x):\n",
        "\n",
        "    # On crée un dossier de sortie pour stocker les résultats de l'expérience 5.2\n",
        "    out_dir = Path(\"exp52_results_VF_fashion_CFlow\")\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "    csv_path = out_dir / \"exp52_results_VF_fashion_CFlow.csv\"\n",
        "\n",
        "    # On initialise le CSV une seule fois\n",
        "    if not csv_path.exists():\n",
        "        with open(csv_path, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                \"model_family\", \"q_star\", \"logp\",\n",
        "                \"Lq\", \"Lq_star\",\n",
        "                \"approx_gap\", \"amort_gap\", \"infer_gap\"\n",
        "            ])\n",
        "\n",
        "    # On définit les trois familles amorties qu'on veut entraîner et comparer\n",
        "    models = [(\"FFG\", Gaussian), (\"Flow\", Flow), (\"ContextualFlow\", ContextualFlow)]\n",
        "\n",
        "    # On fixe un subset aléatoire mais reproductible (seed=0) pour l'évaluation des gaps\n",
        "    rng = np.random.default_rng(0)\n",
        "    subset = train_x[rng.choice(len(train_x), size=15, replace=False)]\n",
        "\n",
        "    # On boucle sur chaque famille (modèle amorti) : FFG, Flow, ContextualFlow\n",
        "    for name, q_class in models:\n",
        "\n",
        "        # On construit le VAE avec la famille q(z|x) choisie, puis on l'entraîne\n",
        "        model = build_vae(q_class)\n",
        "        model = train_model(model, train_x, epochs=500)\n",
        "\n",
        "        # On estime log p(x) avec deux estimateurs : IWAE et AIS\n",
        "        # Puis on prend le max (comme dans ton setup précédent)\n",
        "        logp_iwae = estimate_logp_IWAE(model, subset, K=10000)\n",
        "        logp_ais  = estimate_logp_AIS(model, subset, K=100, T=500)\n",
        "\n",
        "        logp = max(logp_iwae, logp_ais if not np.isnan(logp_ais) else -1e9)\n",
        "\n",
        "        # On calcule la borne amortie L[q] (avec l'encodeur appris)\n",
        "        Lq = amortized_elbo(model, subset)\n",
        "\n",
        "        # On calcule ensuite les bornes localement optimisées L[q*] pour trois familles possibles\n",
        "        # (même si le modèle amorti n'est pas de cette famille)\n",
        "        Lqs_FFG = locally_optimized_elbo(model, subset, Gaussian)\n",
        "        Lqs_Flow = locally_optimized_elbo(model, subset, Flow)\n",
        "        Lqs_CFlow = locally_optimized_elbo(model, subset, ContextualFlow)\n",
        "\n",
        "        # On calcule les trois gaps à partir de (logp, Lq, Lq*)\n",
        "        # approx_gap = logp - Lq*\n",
        "        # amort_gap  = Lq* - Lq\n",
        "        # infer_gap  = logp - Lq\n",
        "        def gaps(logp, Lq, Lqs):\n",
        "            return round(logp - Lqs,2), round(Lqs - Lq,2), round(logp - Lq,2)\n",
        "\n",
        "        g_FFG  = gaps(logp, Lq, Lqs_FFG)\n",
        "        g_Flow = gaps(logp, Lq, Lqs_Flow)\n",
        "        g_CFlow = gaps(logp, Lq, Lqs_CFlow)\n",
        "\n",
        "        # On affiche un résumé pour vérifier que tout est cohérent\n",
        "        print(\"\\n=== Résultats ===\")\n",
        "        print(f\"log p̂(x)     = {logp:.3f}\")\n",
        "        print(f\"L[q]         = {Lq:.3f}\")\n",
        "        print(f\"L[q*_FFG]    = {Lqs_FFG:.3f}\")\n",
        "        print(f\"L[q*_Flow]   = {Lqs_Flow:.3f}\")\n",
        "        print(f\"L[q*_CFlow]  = {Lqs_CFlow:.3f}\")\n",
        "        print(f\"Gaps FFG     = {g_FFG}\")\n",
        "        print(f\"Gaps Flow    = {g_Flow}\")\n",
        "        print(f\"Gaps CFlow   = {g_CFlow}\")\n",
        "\n",
        "        # On sauvegarde les résultats dans le CSV :\n",
        "        # 3 lignes par modèle (q* = FFG, Flow, ContextualFlow)\n",
        "        with open(csv_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([name, \"FFG\",  round(logp,2), round(Lq,2), round(Lqs_FFG,2),  *g_FFG])\n",
        "            writer.writerow([name, \"Flow\", round(logp,2), round(Lq,2), round(Lqs_Flow,2), *g_Flow])\n",
        "            writer.writerow([name, \"ContextualFlow\", round(logp,2), round(Lq,2), round(Lqs_CFlow,2), *g_CFlow])\n",
        "\n",
        "run_exp_5_2(train_x, test_x)\n",
        "print(\" Résultats sauvés dans exp52_results_VF_fashion_CFlow/exp52_results_VF_fashion_CFlow.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
